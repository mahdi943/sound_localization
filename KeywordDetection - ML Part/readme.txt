Hello,

First, the requirements are listed in the "requirements.txt" file.

Here we have 1 python script, 2 notebooks, one raw dataset and one preprocessed dataset.

In the utilities.py script, we have helper functions for audio manipulation.

In the /dataset directory, we have the raw audio dataset that contains audio clips of either background noises or positive/negative samples.

In the /XY_train and /XY_dev directories, we have the preprocessed dataset, which is generated by synthesizing these raw audio clips.
(GitHub didn't allow to upload the dataset, so here's a drive link for the dataset:
https://drive.google.com/drive/folders/1QBJw8TJ6XKEZp2SSN9cPTXDp_F7uC5kQ?usp=sharing)

In the notebook "keyword_detection.ipynb", we have the code for data preparation (audio synthesizing), architecture of the model, building, training and evaluating the model and making predictions on static audio clips.
Inside this notebook, we also have a lot of comments and explanations that describe our methodology.

In the second notebook ("realtime_demo.ipynb"), we have the code for making prediction on the real-time audio stream received by the user.

Finally, you can find the chiming sound that our model adds upon keyword detection at the directory "./audio_examples/chime.wav".
