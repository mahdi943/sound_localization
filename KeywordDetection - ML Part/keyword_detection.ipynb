{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "from utilities import *\n",
    "from scipy.io import wavfile\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Synthesis\n",
    "\n",
    "First, we need to synthesize an audio dataset to train our model. Our keyword will be \"activate\", because there's already a dataset of audio clips with this keyword; therefore, we chose this keyword to ease our job.\n",
    "\n",
    "In our case, we would like to detect the word \"activate\" in all kinds of environments (library, home, offices, open-spaces ...). Since we don't have enough time and resources to take audio clips at this many locations, we synthesize different audio clips to create a dataset. We use recordings with a mix of positive words (\"activate\") and negative words (random words other than activate) on different background sounds.\n",
    "\n",
    "In the /dataset directory, there is a subset of the raw audio files of the positive words (\"activate\"), negative words, and background noise. We use these audio files to synthesize a dataset to train the model. There is one word per audio recording. The \"backgrounds\" directory contains 10-second clips of background noise in different environments.\n",
    "\n",
    "To synthesize a single training example, we\n",
    "\n",
    "- Pick a random 10-second background audio clip (see ./dataset/backgrounds directory)\n",
    "- Randomly insert 0-4 audio clips of \"activate\" into this 10sec clip (see ./dataset/activates directory)\n",
    "- Randomly insert 0-2 audio clips of negative words into this 10sec clip (see ./dataset/negatives directory)\n",
    "\n",
    "We use the pydub package to manipulate audio. Pydub converts raw audio files into lists of Pydub data structures (it is not important to know the details here). Pydub uses 1ms as the discretization interval (1ms is 1 millisecond = 1/1000 seconds) which is why a 10sec clip is always represented using 10,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load audio segments using pydub\n",
    "activates, negatives, backgrounds = load_raw_audio()\n",
    "\n",
    "print(\"background len: \" + str(len(backgrounds[0])))    # Should be 10,000, since it is a 10 sec clip\n",
    "print(\"activate[0] len: \" + str(len(activates[0])))     # Maybe around 1000, since an \"activate\" audio clip is usually around 1 sec (but varies a lot)\n",
    "print(\"activate[1] len: \" + str(len(activates[1])))     # Different \"activate\" clips can have different lengths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Overlaying positive/negative words on the background**:\n",
    "\n",
    "Given a 10sec background clip and a short audio clip (positive or negative word), we will insert the word's short audio clip onto the background. To ensure audio segments inserted onto the background do not overlap, we keep track of the times of previously inserted audio clips. When we insert a 1sec \"activate\" onto a 10sec clip of cafe noise, we end up with a 10sec clip that sounds like someone saying \"activate\" in a cafe, with \"activate\" superimposed on the background cafe noise.\n",
    "\n",
    "\n",
    "**Creating the labels**:\n",
    "\n",
    "The labels $y^{\\langle t \\rangle}$ represent whether someone has just finished saying \"activate.\" Given a background clip, we initialize $y^{\\langle t \\rangle}=0$ for all $t$, since the clip doesn't contain any \"activates.\"\n",
    "\n",
    "When we insert an \"activate\" clip, we also update labels for $y^{\\langle t \\rangle}$, so that 50 steps of the output now have target label 1. We will train a GRU model to detect when someone has *finished* saying \"activate\". For example, suppose the synthesized \"activate\" clip ends at the 5sec mark in the 10sec audio---exactly halfway into the clip. Recall that $T_y = 1375$, so timestep $687 = $ `int(1375*0.5)` corresponds to the moment at 5sec into the audio. So, we will set $y^{\\langle 688 \\rangle} = 1$. Further, we want the GRU to detect the keyword anywhere within a short time-interval after this moment, so we actually set 50 consecutive values of the label $y^{\\langle t \\rangle}$ to 1. Particularly, we have $y^{\\langle 688 \\rangle} = y^{\\langle 689 \\rangle} = \\cdots = y^{\\langle 737 \\rangle} = 1$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From audio recordings to spectrograms\n",
    "\n",
    "We use audio sampled at 44100 Hz. This means the microphone gives us 44100 numbers per second. Thus, a 10 second audio clip is represented by 441000 numbers. It is hard to deal with this many numbers, and to extract the information about whether the keyword \"activate\" is said or not. Hence, to improve the performance of our model, we use spectrograms of the audio clips. A spectrogram simply tells us how much different frequencies are present in an audio clip at a moment in time. It calculates the most active frequencies for each window frame over the raw audio signal using a Fourier transform. Though, we will not mention the mathematical details here.\n",
    "\n",
    "For simplicity, we work on 10-second audio clips we generate using data synthesis; therefore, by default settings, our spectrograms will have 5511 timesteps, and this will be the size of the input of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_, data = wavfile.read(\"audio_examples/example_train.wav\")\n",
    "print(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\n",
    "print(\"Time steps in input after spectrogram\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 5511 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram\n",
    "Ty = 1375 # The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code Part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_ms -- the duration of the audio clip in ms (\"ms\" stands for \"milliseconds\")\n",
    "    \n",
    "    Returns:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) in ms\n",
    "    \"\"\"\n",
    "    segment_start = np.random.randint(low=0, high=10000-segment_ms)   # Make sure segment doesn't run past the 10sec background \n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    return (segment_start, segment_end)\n",
    "\n",
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \"\"\"\n",
    "    Checks if the time of a segment overlaps with the times of existing segments.\n",
    "\n",
    "    Arguments:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) for the new segment\n",
    "    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments\n",
    "\n",
    "    Returns:\n",
    "    True if the time segment overlaps with any of the existing segments, False otherwise\n",
    "    \"\"\"\n",
    "    segment_start, segment_end = segment_time\n",
    "    overlap = False\n",
    "    for previous_start, previous_end in previous_segments:\n",
    "        if segment_start <= previous_end and segment_end >= previous_start:\n",
    "            overlap = True\n",
    "    return overlap\n",
    "\n",
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the\n",
    "    audio segment does not overlap with existing segments.\n",
    "\n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording.\n",
    "    audio_clip -- the audio clip to be inserted/overlaid.\n",
    "    previous_segments -- times where audio segments have already been placed\n",
    "\n",
    "    Returns:\n",
    "    new_background -- the updated background audio\n",
    "    \"\"\"\n",
    "    # Get the duration of the audio clip in ms\n",
    "    segment_ms = len(audio_clip)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "    while is_overlapping(segment_time, previous_segments):\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "    previous_segments.append(segment_time)\n",
    "    new_background = background.overlay(audio_clip, position = segment_time[0])\n",
    "    return new_background, segment_time\n",
    "\n",
    "def insert_ones(y, segment_end_ms):\n",
    "    \"\"\"\n",
    "    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment\n",
    "    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the\n",
    "    50 following labels should be ones.\n",
    "\n",
    "    Arguments:\n",
    "    y -- numpy array of shape (1, Ty), the labels of the training example\n",
    "    segment_end_ms -- the end time of the segment in ms\n",
    "\n",
    "    Returns:\n",
    "    y -- updated labels\n",
    "    \"\"\"\n",
    "    # duration of the background (in terms of spectrogram time-steps)\n",
    "    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n",
    "    # Add 1 to the correct index in the background label (y)\n",
    "    for i in range(segment_end_y + 1, segment_end_y + 51):\n",
    "        if i < Ty:\n",
    "            y[0, i] = 1\n",
    "    return y\n",
    "\n",
    "def create_training_example(background, activates, negatives):\n",
    "    \"\"\"\n",
    "    Creates a training example with a given background, activates, and negatives.\n",
    "\n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording\n",
    "    activates -- a list of audio segments of the word \"activate\"\n",
    "    negatives -- a list of audio segments of random words that are not \"activate\"\n",
    "\n",
    "    Returns:\n",
    "    x -- the spectrogram of the training example\n",
    "    y -- the label at each time step of the spectrogram\n",
    "    \"\"\"\n",
    "    # Set the random seed\n",
    "    np.random.seed(18)\n",
    "\n",
    "    # Make background quieter\n",
    "    background = background - 20\n",
    "\n",
    "    # Initialize y (label vector) of zeros\n",
    "    y = np.zeros((1, Ty))\n",
    "\n",
    "    # Initialize segment times as empty list\n",
    "    previous_segments = []\n",
    "\n",
    "    # Select 0-4 random \"activate\" audio clips from the entire list of \"activates\" recordings\n",
    "    number_of_activates = np.random.randint(0, 5)\n",
    "    random_indices = np.random.randint(len(activates), size=number_of_activates)\n",
    "    random_activates = [activates[i] for i in random_indices]\n",
    "\n",
    "    # Loop over randomly selected \"activate\" clips and insert in background\n",
    "    for random_activate in random_activates:\n",
    "        # Insert the audio clip on the background\n",
    "        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n",
    "        # Retrieve segment_start and segment_end from segment_time\n",
    "        segment_start, segment_end = segment_time\n",
    "        # Insert labels in \"y\"\n",
    "        y = insert_ones(y, segment_end_ms=segment_end)\n",
    "\n",
    "    # Select 0-2 random negatives audio recordings from the entire list of \"negatives\" recordings\n",
    "    number_of_negatives = np.random.randint(0, 3)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    random_negatives = [negatives[i] for i in random_indices]\n",
    "\n",
    "    # Loop over randomly selected negative clips and insert in background\n",
    "    for random_negative in random_negatives:\n",
    "        # Insert the audio clip on the background\n",
    "        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n",
    "\n",
    "    # Standardize the volume of the audio clip\n",
    "    background = match_target_amplitude(background, -20.0)\n",
    "\n",
    "    # Export new training example\n",
    "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n",
    "\n",
    "    # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)\n",
    "    x = graph_spectrogram(\"train.wav\")\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full training set\n",
    "\n",
    "We have now implemented the code needed to generate a single training example. We used this process to generate a large training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed training examples\n",
    "X = np.load(\"./XY_train/X.npy\")\n",
    "Y = np.load(\"./XY_train/Y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Development set\n",
    "\n",
    "To test our model, we have a development set of 25 examples. These instances are handcrafted, meaning that they are created by real audio, instead of synthesized audio clips for making the testing similar to the real life case.\n",
    "\n",
    "This development set is not created by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dev set examples\n",
    "X_dev = np.load(\"./XY_dev/X_dev.npy\")\n",
    "Y_dev = np.load(\"./XY_dev/Y_dev.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model\n",
    "\n",
    "Our model use 1-D convolutional layers, GRU layers, and dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Build the model\n",
    "\n",
    "Here is the architecture we will use. This is a well-known architecture\n",
    "\n",
    "<img src=\"images/model_architecture.png\" style=\"width:500px;height:500px;\">\n",
    "\n",
    "One key step of this model is the 1D convolutional step. It inputs the 5511 step spectrogram, and outputs a 1375 step output, which is then further processed by multiple layers to get the final $T_y = 1375$ step output. This layer plays a role similar to the 2D convolutions that *extract low-level features* and then possibly generating an output of a smaller dimension. We expect from this layer to extract the low-level audio features from the input and also to lower the dimension to make the rest of the computations in the model more efficient.\n",
    "\n",
    "Computationally, the 1-D conv layer also helps speed up the model because now the GRU has to process only 1375 timesteps rather than 5511 timesteps. The two GRU layers read the sequence of inputs from left to right, then ultimately uses a dense+sigmoid layer to make a prediction for $y^{\\langle t \\rangle}$. Because $y$ is binary valued (0 or 1), we use a *sigmoid* output at the last layer to estimate the chance of the output being 1, corresponding to the user having just said \"activate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    # CONV layer\n",
    "    X = Conv1D(196, kernel_size=15, strides=4)(X_input)         # CONV1D\n",
    "    X = BatchNormalization()(X)                                 # Batch normalization\n",
    "    X = Activation('relu')(X)                                 # ReLu activation\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "\n",
    "    # First GRU Layer\n",
    "    X = GRU(units = 128, return_sequences = True)(X)    # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    X = BatchNormalization()(X)                                 # Batch normalization\n",
    "    \n",
    "    # Second GRU Layer\n",
    "    X = GRU(units = 128, return_sequences = True)(X)    # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    X = BatchNormalization()(X)                                  # Batch normalization\n",
    "    X = Dropout(0.8)(X)                                  # dropout (use 0.8)\n",
    "    \n",
    "    # Time-distributed dense layer\n",
    "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(input_shape = (Tx, n_freq))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the network is of shape (None, 1375, 1) while the input is (None, 5511, 101). The Conv1D has reduced the number of steps from 5511 at spectrogram to 1375."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training takes a long time to train. To save time, we use a pretrained model on a large training set of about 4000 examples. Let's load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/tr_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train the model further, using the Adam optimizer and binary cross entropy loss, as follows. This will run quickly because we are training just for one epoch and with a small training set of 26 examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, batch_size = 5, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Test the model\n",
    "\n",
    "Finally, let's see how your model performs on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dev set accuracy =  0.9502545595169067\n",
    "\n",
    "Which looks satisfying."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Making Predictions\n",
    "\n",
    "Let's use it to make predictions. This code snippet runs audio (saved in a wav file) through the network. In the second notebook, you can see the implementation that feeds continous audio stream input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword(filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "\n",
    "    x = graph_spectrogram(filename)\n",
    "    # the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions\n",
    "\n",
    "def chime_on_activate(filename, predictions, threshold):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    Ty = predictions.shape[1]\n",
    "    # Step 1: Initialize the number of consecutive output steps to 0\n",
    "    consecutive_timesteps = 0\n",
    "    # Step 2: Loop over the output steps in the y\n",
    "    for i in range(Ty):\n",
    "        # Step 3: Increment consecutive output steps\n",
    "        consecutive_timesteps += 1\n",
    "        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
    "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
    "            # Step 5: Superpose audio and background using pydub\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
    "            # Step 6: Reset consecutive output steps to 0\n",
    "            consecutive_timesteps = 0\n",
    "\n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')\n",
    "\n",
    "chime_file = \"audio_examples/chime.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we estimate the probability of having detected the word \"activate\" at each output step, we trigger a \"chiming\" sound to play when the probability is above a certain threshold. *Normally, this is the part where we should integrate this ML part with the robotic part of the project, but there was not enough time.*\n",
    "\n",
    "Further, $y^{\\langle t \\rangle}$ might be near 1 for many values in a row after \"activate\" is said, yet we want to chime only once. So we will insert a chime sound at most once every 75 output steps. This will help prevent us from inserting two chimes for a single instance of \"activate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run the model on the audio clips and see if it adds a chime after \"activate\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio to the correct format\n",
    "def preprocess_audio(filename):\n",
    "    # Trim or pad audio segment to 10000ms\n",
    "    padding = AudioSegment.silent(duration=10000)\n",
    "    segment = AudioSegment.from_wav(filename)[:10000]\n",
    "    segment = padding.overlay(segment)\n",
    "    # Set frame rate to 44100\n",
    "    segment = segment.set_frame_rate(44100)\n",
    "    # Export as wav\n",
    "    segment.export(filename, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_filename = \"audio_examples/my_audio.wav\"\n",
    "preprocess_audio(your_filename)\n",
    "IPython.display.Audio(your_filename)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "rSupZ",
   "launcher_item_id": "cvGhe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
